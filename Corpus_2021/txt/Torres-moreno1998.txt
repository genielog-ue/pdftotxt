Nom du PDF : Torres-moreno1998.pdf
Titre du PDF : Efficient Adaptive Learning for Classification Tasks with Binary Units
Auteur du PDF : joe@next.castanet.com Joe Pickert
Email : 
Abstract : Communicated by Scott Fahlman

Efficient Adaptive Learning for Classification Tasks with
Binary Units
J. Manuel Torres Moreno
Mirta B. Gordon
DeÃÅpartement de Recherche Fondamentale sur la MatieÃÄre CondenseÃÅe, CEA Grenoble,
38054 Grenoble Cedex 9, France

This article presents a new incremental learning algorithm for classification tasks, called NetLines, which is well adapted for both binary
and real-valued input patterns. It generates small, compact feedforward
neural networks with one hidden layer of binary units and binary output
units. A convergence theorem ensures that solutions with a finite number of hidden units exist for both binary and real-valued input patterns.
An implementation for problems with more than two classes, valid
for any binary classifier, is proposed. The generalization error and
the size of the resulting networks are compared to the best published
results on well-known classification benchmarks. Early stopping is shown
to decrease overfitting, without improving the generalization performance.


References :  are the same as in Figure 2.

samples of breast cytology, classified as benign or malignant. We excluded
from the original database 16 patterns that have the attribute Œæ6 (‚Äúbare nuclei‚Äù) missing. Among the remaining D = 683 patterns, the two classes are
unevenly represented, 65.5% of the examples being benign. We studied the
generalization performance of networks trained with sets of several sizes P.
The P patterns for each learning test were selected at random. In Figure 4a,
the generalization error at classifying the remaining G ‚â° D ‚àí P patterns is
displayed as a function of the corresponding number of weights in a logarithmic scale. For comparison, we included in the same figure results of a
single perceptron trained with P = 75 patterns using Minimerror. The results, averaged values over 50 independent tests for each P, show that both
NetLines and MonoPlane have lower ¬≤g and fewer parameters than other
algorithms on this benchmark.
The total number of weights updates needed by NetLines, including the
weights of the dropped output units, is 7 ¬∑ 104 ; backpropagation needed
‚âà 104 (Prechelt, 1994).
The trained network may be used to classify the patterns with missing
attributes. The number of misclassified patterns among the 16 cases for
which attribute Œæ6 is missing is plotted as a function of the possible values
of Œæ6 on Figure 4b. For large values of Œæ6 , there are discrepancies between the
medical and the network‚Äôs diagnosis on half the cases. This is an example
of the kind of information that may be obtained in practical applications.

Classification Tasks with Binary Units

1023



%UHDVW FDQFHU D

%UHDVW FDQFHU E

0LQLPHUURU 3 



0RQR3ODQH



1HW/LQHV 3 



1HW/LQHV




ŒµJ




0RQR3ODQH 3 








1HW/LQHV
0RQR3ODQH






1XPEHU RI ZHLJKWV

















3RVVLEOH YDOXHV RI DWWULEXWH





Œæ

Figure 4: Breast cancer classification. (a) Generalization error ¬≤g versus number of weights (logarithmic scale), for P = 525. 1‚Äì3: Rprop with no shortcuts
(Prechelt, 1994); 4‚Äì6: Rprop with shortcuts (Prechelt, 1994); 7: Cascade Correlation (Depenau, 1995). For comparison, results with smaller training sets, P = 75
(single perceptron) and P = 160, are displayed. Results of MonoPlane and NetLines are averages over 50 tests. (b) Classification errors versus possible values
of the missing attribute bare nuclei for the 16 incomplete patterns, averaged
over 50 independently trained networks.

5.2.2 Diabetes Diagnosis. This benchmark (Prechelt, 1994) contains D =
768 patterns described by N = 8 real-valued attributes, corresponding to
‚âà 35% of Pima women suffering from diabetes, 65% being healthy. Training
sets of P = 576 patterns were selected at random, and generalization was
tested on the remaining G = 192 patterns. The comparison with published
results obtained with other algorithms tested under the same conditions,
presented in Figure 5, shows that NetLines reaches the best performance
published so far on this benchmark, needing many fewer parameters. Training times of NetLines are of ‚âà 105 updates. The numbers of updates needed
by Rprop (Prechelt, 1994) range between 4 ¬∑ 103 and 5 ¬∑ 105 , depending on
the network‚Äôs architecture.
5.3 Multiclass Problems. We applied our learning algorithm to two different problems, both of three classes. We compare the results obtained with
a WTA classification based on the results of three networks, each independently trained to separate one class from the two others, to the results of
the TON architectures described in section 4. Because the number of classes
is low, we determined the three TONs, corresponding to the three possible

1024

J. Manuel Torres Moreno and Mirta B. Gordon



,QGLDQV 3LPD 'LDEHWHV








ŒµJ








1HW/LQHV








1XPEHU RI ZHLJKWV

Figure 5: Diabetes diagnosis: Generalization error ¬≤g versus number of weights.
Results of NetLines are averages over 50 tests. 1‚Äì3: Rprop no shortcuts, 4‚Äì6:
Rprop with shortcuts (Prechelt, 1994).

learning sequences. The vote of the three TONs improves the performances,
as expected.
5.3.1 Breiman‚Äôs Waveform Recognition Problem. This problem was introduced as a test for the algorithm CART (Breiman et al., 1984). The input
patterns are defined by N = 21 real-valued amplitudes x(t) observed at regularly spaced intervals t = 1, 2, . . . , N. Each pattern is a noisy convex linear
combination of two among three elementary waves (triangular waves centered on three different values of t). There are three possible combinations,
and the pattern‚Äôs class identifies from which combination it is issued.
We trained the networks with the same 11 training sets of P = 300 examples, and generalization was tested on the same independent test set
of G = 5000, as in Gascuel (1995). Our results are displayed in Figure 6,
where only results of algorithms reaching ¬≤g < 0.25 in Gascuel (1995) are
included. Although it is known that due to the noise, the classification error
has a lower bound of ‚âà 14% (Breiman et al., 1984), the results of NetLines
and MonoPlane presented here correspond to error-free training. The networks generated by NetLines have between three and six hidden neurons,
depending on the training sets. The results obtained with a single perceptron trained with Minimerror and with the perceptron learning algorithm,
which may be considered the extreme case of early stopping, are hardly improved by the more complex networks. Here again the overfitting produced
by error-free learning with NetLines does not cause the generalization per-

Classification Tasks with Binary Units

1025


%UHLPDQ V :DYHIRUPV





0RQR3ODQH :7$








ŒµJ











0LQLPHUURU
1HW/LQHV 9RWH



7KHRUHWLFDO OLPLW











1XPEHU RI SDUDPHWHUV

Figure 6: Breiman waveforms: Generalization error ¬≤g averaged over 11 tests
versus number of parameters. Error bars on the number of weights generated
by NetLines and MonoPlane are not visible at the scale of the figure. 1: linear discrimination; 2: perceptron; 3: backpropagation; 4: genetic algorithm; 5: quadratic
discrimination; 6: Parzen‚Äôs kernel; 7: K-NN; 8: constraint (Gascuel, 1995).

formance to deteriorate. The TONs vote reduces the variance but does not
decrease the average ¬≤g .
5.3.2 Fisher‚Äôs Iris Plants Database. In this classic three-class problem, one
has to determine the class of iris plants based on the values of N = 4 realvalued attributes. The database of D = 150 patterns contains 50 examples
of each class. Networks were trained with P = 149 patterns, and the generalization error is the mean value of all the 150 leave-one-out possible tests.
Results of ¬≤g are displayed as a function of the number of weights in Figure 7.
Error bars are available for only our own results. In this difficult problem,
the vote of the three possible TONs trained with the three possible class
sequences (see section 4) improves the generalization performance.
6 Conclusion
We presented an incremental learning algorithm for classification, which we
call NetLines. It generates small feedforward neural networks with a single
hidden layer of binary units connected to a binary output neuron. NetLines
allows for an automatic adaptation of the neural network to the complexity
of the particular task. This is achieved by coupling an error-correcting strategy for the successive addition of hidden neurons with Minimerror, a very

1026

J. Manuel Torres Moreno and Mirta B. Gordon



,5,6 GDWDEDVH







1HW/LQHV :7$

ŒµJ










1HW/LQHV YRWH






1XPEHU RI ZHLJKWV

Figure 7: Iris database: Generalization error ¬≤g versus number of parameters.
1: offset, 2: backpropagation (Martinez & EsteÃÄve, 1992); 4,5: backpropagation
(Verma & Mulawka, 1995); 3,6: gradient-descent orthogonalized training (Verma
& Mulawka, 1995).

efficient perceptron training algorithm. Learning is fast not only because
it reduces the problem to that of training single perceptrons, but mainly
because there is no longer a need for the usual preliminary tests required to
determine the correct architecture for the particular application. Theorems
valid for binary as well as for real-valued inputs guarantee the existence of
a solution with a bounded number of hidden neurons obeying the growth
strategy.
The networks are composed of binary hidden units whose states constitute a faithful encoding of the input patterns. They implement a mapping
from the input space to a discrete H-dimensional hidden space, H being
the number of hidden neurons. Thus, each pattern is labeled with a binary
word of H bits. This encoding may be seen as a compression of the pattern‚Äôs
information. The hidden neurons define linear boundaries, or portions of
boundaries, between classes in input space. The network‚Äôs output may be
given a probabilistic interpretation based on the distance of the patterns to
these boundaries.
Tests on several benchmarks showed that the networks generated by our
incremental strategy are small, in spite of the fact that the hidden neurons
are appended until error-free learning is reached. Even when the networks
obtained with NetLines are larger than those used by other algorithms, its
generalization error remains among the smallest values reported. In noisy
or difficult problems, it may be useful to stop the network‚Äôs growth before

Classification Tasks with Binary Units

1027

the condition of zero training errors is reached. This decreases overfitting, as
smaller networks (with less parameters) are thus generated. However, the
prediction quality (measured by the generalization error) of the classifiers
generated with NetLines is not improved by early stopping.
Our results were obtained without cross-validation or any data manipulation like boosting, bagging, or arcing (Breiman, 1994; Drucker, Schapire,
& Simard, 1993). Those costly procedures combine results of very large
numbers of classifiers, with the aim of improving the generalization performance through the reduction of the variance. Because NetLines is a stable
classifier, presenting small variance, we do not expect that such techniques
would significantly improve our results.
Appendix
In this appendix we exhibit a particular solution to the learning strategy of
NetLines. This solution is built in such a way that the cardinal of a convex
subset of well-learned patterns, Lh , grows monotonically upon the addition
of hidden units. Because this cardinal cannot be larger than the total number
of training patterns, the algorithm must stop with a finite number of hidden
units.
Suppose that h hidden units have already been included and that the
output neuron still makes classification errors on patterns of the training set,
called training errors. Among these wrongly learned patterns, let ŒΩ be the
E h , called hyperplane-h hereafter.
one closest to the hyperplane normal to w
We define Lh as the subset of (correctly learned) patterns lying closer to
hyperplane-h than ŒæE ŒΩ . Patterns in Lh have 0 < Œ≥h < |Œ≥hŒΩ |. The subset Lh and
at least pattern ŒΩ are well learned if the next hidden unit, h + 1, has weights:
E h ¬∑ ŒæE ŒΩ )eÃÇ0 ,
E h ‚àí (1 ‚àí ¬≤h )œÑhŒΩ (w
E h+1 = œÑhŒΩ w
w

(A.1)

where eÃÇ0 ‚â° (1, 0, . . . , 0). The conditions that both Lh and pattern ŒΩ have
positive stabilities (are correctly learned) impose that
¬µ

0 < ¬≤h < min
¬µ‚ààLh

|Œ≥hŒΩ | ‚àí Œ≥h
.
|Œ≥hŒΩ |

(A.2)

The following weights between the hidden units and the output will give
the correct output to pattern ŒΩ and to the patterns of Lh :
W0 (h + 1) = W0 (h) + œÑ ŒΩ
Wi (h + 1) = Wi (h) for 1 ‚â§ i ‚â§ h
Wh+1 (h + 1) = ‚àíœÑ ŒΩ .

(A.3)
(A.4)
(A.5)

Thus, card(Lh+1 ) ‚â• card(Lh ) + 1. As the number of patterns in Lh increases
monotonically with h, convergence is guaranteed with less than P hidden
units.

1028

J. Manuel Torres Moreno and Mirta B. Gordon

Acknowledgments
J.M. thanks Consejo Nacional de Ciencia y Tecnologƒ±ÃÅa and Universidad
AutoÃÅnoma Metropolitana, MeÃÅxico, for financial support (grant 65659).
References
Alpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Unpublished doctoral dissertation, Ecole Polytechnique FeÃÅdeÃÅrale de Lausanne,
Switzerland.
Biehl, M., & Opper, M. (1991). Tilinglike learning in the parity machine. Physical
Review A, 44, 6888.
Bottou, L., & Vapnik, V. (1992). Local learning algorithms. Neural Computation,
4(6), 888‚Äì900.
Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Department
of Statistics, University of California at Berkeley.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification
and regression trees. Monterey, CA: Wadsworth and Brooks/Cole.
Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., & Hopfield, J.
(1987). Large automatic learning, rule extraction, and generalization. Complex
Systems, 1, 877‚Äì922.
Depenau, J. (1995). Automated design of neural network architecture for classification.
Unpublished doctoral dissertation, Computer Science Department, Aarhus
University.
Drucker, H., Schapire, R., & Simard, P. (1993). Improving performance in neural networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &
C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 42‚Äì
49). San Mateo, CA: Morgan Kaufmann.
Fahlman, S. E., & Lebiere, C. (1990). The cascade-correlation learning architecture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,
2 (pp. 524‚Äì532). San Mateo: Morgan Kaufmann.
Farrell, K. R., & Mammone, R. J. (1994). Speaker recognition using neural tree
networks. In J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural
Information Processing Systems, 6 (pp. 1035‚Äì1042). San Mateo, CA: Morgan
Kaufmann.
Frean, M. (1990). The Upstart algorithm: A method for constructing and training
feedforward neural networks. Neural Computation, 2(2), 198‚Äì209.
Frean, M. (1992). A ‚Äúthermal‚Äù perceptron learning rule. Neural Computation, 4(6),
946‚Äì957.
Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality
(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.
Fritzke, B. (1994). Supervised learning with growing cell structures. In
J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in neural information processing systems, 6 (pp. 255‚Äì262). San Mateo, CA: Morgan Kaufmann.
Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. Pattern
Recognition, Oct. 28‚Äì31, Paris, vol. 4.

Classification Tasks with Binary Units

1029

Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.).
5eÃÄmes JourneÃÅes Nationales du PRC-IA Teknea, Nancy.
Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the
bias/variance dilemma. Neural Computation, 4(1), 1‚Äì58.
Goodman, R. M., Smyth, P., Higgins, C. M., & Miller, J. W. (1992). Rule-based
neural networks for classification and probability estimation. Neural Computation, 4(6), 781‚Äì804.
Gordon, M. B. (1996). A convergence theorem for incremental learning with realvalued inputs. In IEEE International Conference on Neural Networks, pp. 381‚Äì
386.
Gordon, M. B., & Berchier, D. (1993). Minimerror: A perceptron learning rule
that finds the optimal weights. In M. Verleysen (Ed.), European Symposium on
Artificial Neural Networks (pp. 105‚Äì110). Brussels: D Facto.
Gordon, M. B., & Grempel, D. (1995). Optimal learning with a temperature
dependent algorithm. Europhysics Letters, 29(3), 257‚Äì262.
Gordon, M. B., Peretto, P., & Berchier, D. (1993). Learning algorithms for perceptrons from statistical physics. Journal of Physics I (France), 3, 377‚Äì387.
Gorman, R. P., & Sejnowski, T. J. (1988). Analysis of hidden units in a layered
network trained to classify sonar targets. Neural Networks, 1, 75‚Äì89.
Gyorgyi, G., & Tishby, N. (1990). Statistical theory of learning a rule. In
W. K. Theumann & R. Koeberle (Eds.), Neural networks and spin glasses. Singapore: World Scientific.
Hoehfeld, M., & Fahlman, S. (1991). Learning with limited numerical precision using
the cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:
Carnegie Mellon University.
Knerr, S., Personnaz, L., & Dreyfus, G. (1990). Single-layer learning revisited: A
stepwise procedure for building and training a neural network. In J. HeÃÅrault
& F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications
(pp. 41‚Äì50). Berlin: Springer-Verlag.
Marchand, M., Golea, M., & RujaÃÅn, P. (1990). A convergence theorem for sequential learning in two-layer perceptrons. Europhysics Letters, 11, 487‚Äì492.
Martinez, D., & EsteÃÄve, D. (1992). The offset algorithm: Building and learning
method for multilayer neural networks. Europhysics Letters, 18, 95‚Äì100.
MeÃÅzard, M., & Nadal, J.-P. (1989). Learning in feedforward layered networks:
The Tiling algorithm. J. Phys. A: Math. and Gen., 22, 2191‚Äì2203.
Mukhopadhyay, S., Roy, A., Kim, L. S., & Govil, S. (1993). A polynomial time algorithm for generating neural networks for pattern classification: Its stability
properties and some test results. Neural Computation, 5(2), 317‚Äì330.
Nadal, J.-P. (1989). Study of a growth algorithm for a feedforward neural network. Int. J. Neur. Syst., 1, 55‚Äì59.
Prechelt, L. (1994). PROBEN1‚ÄîA set of benchmarks and benchmarking rules for neural network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,
Faculty of Informatics.
Raffin, B., & Gordon, M. B. (1995). Learning and generalization with Minimerror,
a temperature dependent learning algorithm. Neural Computation, 7(6), 1206‚Äì
1224.

1030

J. Manuel Torres Moreno and Mirta B. Gordon

Reilly, D. E, Cooper, L. N., & Elbaum, C. (1982). A neural model for category
learning. Biological Cybernetics, 45, 35‚Äì41.
Roy, A., Kim, L., & Mukhopadhyay, S. (1993). A polynomial time algorithm
for the construction and training of a class of multilayer perceptron. Neural
Networks, 6(1), 535‚Äì545.
Sirat, J. A., & Nadal, J.-P. (1990). Neural trees: A new tool for classification.
Network, 1, 423‚Äì438.
Solla, S. A. (1989). Learning and generalization in layered neural networks: The
contiguity problem. In L. Personnaz & G. Dreyfus (Eds.), Neural Networks
from Models to Applications. Paris: I.D.S.E.T.
Torres Moreno, J.-M., & Gordon, M. B. (1995). An evolutive architecture coupled
with optimal perceptron learning for classification. In M. Verleysen (Ed.),
European Symposium on Artificial Neural Networks. Brussels: D Facto.
Torres Moreno, J.-M., & Gordon, M. B. (1998). Characterization of the sonar
signals benchmark. Neural Proc. Letters, 7(1), 1‚Äì4.
Trhun, S. B., et al. (1991). The monk‚Äôs problems: A performance comparison of different
learning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: Carnegie
Mellon University.
Vapnik, V. (1992). Principles of risk minimization for learning theory. In
J. E. Moody, S. J. Hanson, & R. P. Lippmann (Eds.), Advances in neural information processing systems, 4 (pp. 831‚Äì838). San Mateo, CA: Morgan Kaufmann.
Verma, B. K., & Mulawka, J. J. (1995). A new algorithm for feedforward neural networks. In M. Verleysen (Ed.), European Symposium on Artificial Neural
Networks (pp. 359‚Äì364). Brussels: D Facto.
Wolberg, W. H., & Mangasarian, O. L. (1990). Multisurface method of pattern
separation for medical diagnosis applied to breast cytology. In Proceedings of
the National Academy of Sciences, USA, 87, 9193‚Äì9196.
Received February 13, 1997; accepted September 4, 1997.

This article has been cited by:
1. C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectral
analysis. IEEE Transactions on Neural Networks 10, 725-740. [CrossRef]
2. Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward Neural
Networks. International Journal of Neural Systems 08, 647-659. [CrossRef]


